{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**Introduction to NLP with transformers**\n","\n","In this half of the session we will now practice some practical applications of tranformer models for Natural Language Processing (NLP) tasks. This notebook provides examples of NLP with a focus on using large datasets from social media as a novel tool for investigating environmental challenges. However, the methodologies presented here can be replicated using your own datasets. This workshop will be broken into three sections:\n","\n","1. Simple NLP applications using pretrained models\n","2. Sentiment analysis of large text databases using pretrained models\n","3. Topic modeling of large text databases using pretrained models\n","\n","I will be demonstrating simple implementations of pretrained transformer models for various Natural Language Processing (NLP) tasks. Transformers have revolutionized the field of NLP, and pretrained models like BERT, GPT-2, and RoBERTa have shown remarkable performance across a wide range of tasks. In this hands-on session, I will walk you through the process of using these powerful models for tasks such as text classification, named entity recognition, sentiment analysis, and more.\n","\n","Throughout this workshop, you will gain practical experience in:\n","\n","* Loading and Preprocessing Data: I will guide you through the steps of loading and preprocessing text data, ensuring it is in the suitable format for the transformer models.\n","* Implementing Pretrained Transformer Models: You will learn how to leverage popular pretrained transformer models, such as BERT and GPT-2, using libraries like Hugging Face's Transformers. We will explore how to access these models, tokenize input text, and obtain embeddings.\n","* Practical Tips and Best Practices: Throughout the session, I will share practical tips and best practices to enhance your understanding and proficiency in working with pretrained transformer models.\n","\n","Whether you are new to NLP or have some experience, this workshop is designed to provide you with hands-on experience and insights into the capabilities of pretrained transformer models. By the end of this session, you will have the confidence to apply these models to your own NLP projects and leverage their power for various real-world applications.\n","\n","At the start of each section, we will load the required packages (even if loaded before), this way if you want to repeate any section on its own you can start at that point without having to look back to find the correct packages to install and import."],"metadata":{"id":"FF13atmtO4hp"}},{"cell_type":"markdown","source":["#**Section 1. Simple NLP applications using pre-trained models**\n","\n","In this section we will look at a range of differnt NLP methodologies and apply some pretrained models to analyzing small text documents.  In this section, I will introduce you to the exciting world of Natural Language Processing (NLP) and show you how to leverage pre-trained models to perform various NLP tasks effortlessly.\n","\n","NLP is a rapidly evolving field that focuses on teaching machines to understand, interpret, and generate human language. With the advent of pre-trained transformer models like BERT, GPT-3, and others, NLP has witnessed groundbreaking advancements, making it easier than ever to achieve impressive results with minimal effort.\n","\n","Throughout this section, I will guide you step-by-step through simple yet powerful NLP applications. You will learn how to carry out differnt methods including:\n","\n","* Utilize Pre-trained Models: I will introduce you to the concept of pre-trained models and demonstrate how to access and use them through popular NLP libraries.\n","* Text Classification: You will discover how to implement text classification tasks, such as sentiment analysis or topic classification, using pre-trained models.\n","* Named Entity Recognition (NER): I will show you how to extract entities like names, organizations, and locations from text using pre-trained models.\n","* Text Generation: Get ready to witness the magic of language models as we delve into text generation tasks, where you can make the model generate creative and contextually relevant text.\n","* Question Answering: You will learn how to build a question-answering system using pre-trained models, allowing you to find answers to specific questions within a given corpus.\n","\n","By the end of this section, you will have a solid foundation in implementing practical NLP applications using pre-trained models. Whether you are a beginner or already have some experience in NLP, this section will equip you with essential skills to tackle various NLP tasks with confidence."],"metadata":{"id":"4W6CVYYZkRQM"}},{"cell_type":"markdown","source":["First we will need to install the correct Python libraries to carry out this section of the NLP workshop."],"metadata":{"id":"xJP2-mp0CTH8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"bME7h0F_O1Ur"},"outputs":[],"source":["%%capture\n","!pip install transformers"]},{"cell_type":"markdown","source":["Restart the Notebook\n","After installing, some packages that were already loaded were updated and in order to correctly use them, we should now restart the notebook.\n","\n","From the Menu:\n","\n","Runtime → Restart Runtime"],"metadata":{"id":"wjWK7BWh6vYK"}},{"cell_type":"code","source":["from transformers import pipeline"],"metadata":{"id":"tG6sXq313Fhq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##*Task 1. Unmasking words*\n","\n","In the context of Natural Language Processing (NLP), \"unmasking words\" typically refers to a specific task related to language model interpretation and understanding. Specifically, it involves predicting or revealing the masked (hidden) words in a sentence.\n","\n","Unmasking has been widely used in the development and understanding of advanced language models, and it has contributed significantly to the improvement of NLP tasks across various applications.\n","\n","Unmasking words can also be used for model interpretability and analysis. By analyzing the probabilities assigned to different words as candidates for the masked positions, researchers and developers can gain insights into the model's behavior and biases."],"metadata":{"id":"k680hIHbQrnN"}},{"cell_type":"code","source":["#load in the umasker model\n","unmasker = pipeline('fill-mask', model='bert-base-uncased')\n","\n","unmasker(\"Climate [MASK] is caused by greenhouse gases\")"],"metadata":{"id":"MrwSIfFFP3kF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691516197369,"user_tz":240,"elapsed":6462,"user":{"displayName":"Ken Reid","userId":"08628755971426398220"}},"outputId":"60162fdb-7fb7-47a9-c0dd-8a0697e78d31"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'score': 0.9871250987052917,\n","  'token': 2689,\n","  'token_str': 'change',\n","  'sequence': 'climate change is caused by greenhouse gases'},\n"," {'score': 0.005713209975510836,\n","  'token': 28436,\n","  'token_str': 'variability',\n","  'sequence': 'climate variability is caused by greenhouse gases'},\n"," {'score': 0.001469021080993116,\n","  'token': 12959,\n","  'token_str': 'warming',\n","  'sequence': 'climate warming is caused by greenhouse gases'},\n"," {'score': 0.0006010720389895141,\n","  'token': 8386,\n","  'token_str': 'variation',\n","  'sequence': 'climate variation is caused by greenhouse gases'},\n"," {'score': 0.00027156437863595784,\n","  'token': 5213,\n","  'token_str': 'shock',\n","  'sequence': 'climate shock is caused by greenhouse gases'}]"]},"metadata":{},"execution_count":42}]},{"cell_type":"markdown","source":["### *Try it yourself!*"],"metadata":{"id":"jZG7az7_nXND"}},{"cell_type":"code","source":["unmasker(\"YOUR TEXT AND MASK HERE\")"],"metadata":{"id":"EXxhFsJdnXlQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##*Task 2. Sentiment classifier*\n","\n","Sentiment classifiers are machine learning models designed to analyze and classify the sentiment or emotional tone expressed in a piece of text, such as a review, tweet, or customer feedback. The goal of sentiment classification is to determine whether the sentiment expressed is positive, negative, neutral, or sometimes more fine-grained emotions like happy, sad, angry, etc. Here, we will look at both approaches.\n","\n","*In section two we will look at how we can apply sentiment classifcation to a dataset containing multiple text documents.*"],"metadata":{"id":"Qt0R2Dx_RVDE"}},{"cell_type":"code","source":["#load in the sentiment model\n","classifier = pipeline(\"text-classification\",\n","                      model=\"j-hartmann/emotion-english-distilroberta-base\",\n","                      return_all_scores=True)\n","\n","#classify the sentiment of some text\n","classifier([\"I love Elephants!\"])"],"metadata":{"id":"sNfjKl5JRUh0","executionInfo":{"status":"ok","timestamp":1691516201080,"user_tz":240,"elapsed":3716,"user":{"displayName":"Ken Reid","userId":"08628755971426398220"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"533c9889-ce9e-4e63-89a5-585553057327"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[{'label': 'anger', 'score': 0.003975102677941322},\n","  {'label': 'disgust', 'score': 0.001414849073626101},\n","  {'label': 'fear', 'score': 0.0004829037934541702},\n","  {'label': 'joy', 'score': 0.9760998487472534},\n","  {'label': 'neutral', 'score': 0.004919619299471378},\n","  {'label': 'sadness', 'score': 0.0075303311459720135},\n","  {'label': 'surprise', 'score': 0.005577391944825649}]]"]},"metadata":{},"execution_count":43}]},{"cell_type":"markdown","source":["This second exmple is a binary classifier that says whether expressed sentiment is positve or negative."],"metadata":{"id":"qO038_gsqee3"}},{"cell_type":"code","source":["pipe = pipeline(\"text-classification\")\n","pipe(\"This safari is awesome\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5SE-gpT9svPv","executionInfo":{"status":"ok","timestamp":1691516204383,"user_tz":240,"elapsed":3306,"user":{"displayName":"Ken Reid","userId":"08628755971426398220"}},"outputId":"a1b8cf1d-d334-435f-91e5-4041fc027570"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'label': 'POSITIVE', 'score': 0.9998747110366821}]"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["pipe(\"I hate the current air pollution caused by wildfires\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lEjCCM8wqv8t","executionInfo":{"status":"ok","timestamp":1691516204508,"user_tz":240,"elapsed":126,"user":{"displayName":"Ken Reid","userId":"08628755971426398220"}},"outputId":"2f55cf57-c2c6-4c77-d02c-d59ce2c84e18"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'label': 'NEGATIVE', 'score': 0.9965439438819885}]"]},"metadata":{},"execution_count":45}]},{"cell_type":"markdown","source":["### *Try it yourself!*"],"metadata":{"id":"csnojhTXm_Yj"}},{"cell_type":"code","source":["#try using the same phrase twice to see how the two models provide differnt results\n","classifier(\"YOUR TEXT HERE\")\n","pipe(\"YOUR TEXT HERE\")"],"metadata":{"id":"5SSmaHhAm_-J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## *Task 3: Classify a sentence*\n","\n","NLP and sentence classifiers refer to the field of Natural Language Processing and the specific task of classifying sentences into different categories based on their content or meaning. Unlike sentiment classifiers that focus on identifying the sentiment expressed in a sentence (positive, negative, neutral), sentence classifiers aim to categorize sentences into various classes, which may not necessarily be related to sentiment.\n","\n","Sentence classifiers are widely used in various NLP applications to automatically categorize and organize text data, enabling more efficient information retrieval, decision-making, and automation in a range of domains."],"metadata":{"id":"qImmDtW6Whtm"}},{"cell_type":"code","source":["#load in the classification model\n","classifier = pipeline(\"zero-shot-classification\")\n","\n","#classify some text based on what you are looking for\n","classifier(\n","    \"On vacation we stayed in an eco logde, we saw five rhinos.\",\n","    candidate_labels=[\"environment\", \"politics\", \"business\"],\n",")"],"metadata":{"id":"FIvKufy3WDvs","executionInfo":{"status":"ok","timestamp":1691516229513,"user_tz":240,"elapsed":25012,"user":{"displayName":"Ken Reid","userId":"08628755971426398220"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"292647ba-2df9-4a07-ae1b-ba42c73aca29"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n"]},{"output_type":"execute_result","data":{"text/plain":["{'sequence': 'On vacation we stayed in an eco logde, we saw five rhinos.',\n"," 'labels': ['environment', 'business', 'politics'],\n"," 'scores': [0.9846481084823608, 0.009745074436068535, 0.005606801249086857]}"]},"metadata":{},"execution_count":46}]},{"cell_type":"markdown","source":["### *Try it yourself!*"],"metadata":{"id":"lfsq1Rdlmquu"}},{"cell_type":"code","source":["#classify some text based on what you are looking for\n","classifier(\n","    \"YOUR TEXT HERE\",\n","    candidate_labels=[\"YOUR LABEL CHOICES HERE\"],"],"metadata":{"id":"DBbEGMDymrAE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Task 4: Named Entitiy Recognision\n","\n","NER, short for Named Entity Recognition, is a crucial task in Natural Language Processing (NLP) that involves identifying and classifying named entities in a given text. Named entities are real-world objects that have specific names, such as names of people, organizations, locations, dates, time expressions, quantities, and more. The primary goal of NER is to extract and categorize these entities from unstructured text data, enabling better understanding and analysis of the content.\n","\n","Key aspects of Named Entity Recognition (NER):\n","\n","Entity Categories: Named entities can belong to various categories, including but not limited to:\n","* Person: Names of individuals.\n","* Organization: Names of companies, institutions, or other organizations.\n","* Location: Names of cities, countries, regions, and other geographic entities.\n","* Date: Specific dates or periods expressed in the text.\n","* Time: Specific times or time expressions mentioned in the text.\n","* Quantity: Measurements, monetary values, percentages, etc.\n","\n","NER is a fundamental NLP task with numerous applications in information retrieval, document analysis, and natural language understanding. Accurate named entity recognition is a crucial step for extracting valuable insights and knowledge from unstructured text data."],"metadata":{"id":"Z0Yd5riAjtkL"}},{"cell_type":"code","source":["#load in the named entity recognision model\n","ner = pipeline(\"ner\",\n","               grouped_entities=True)\n","\n","#classify some text\n","ner(\"My name is Nathan and I work at The University of Michigan in Michigan, USA.\")"],"metadata":{"id":"Pu3wAmAAjxbl","executionInfo":{"status":"ok","timestamp":1691516244018,"user_tz":240,"elapsed":14515,"user":{"displayName":"Ken Reid","userId":"08628755971426398220"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"15ee5191-ccf6-49ab-b95a-c43a737fd231"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n","Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'entity_group': 'PER',\n","  'score': 0.9986727,\n","  'word': 'Nathan',\n","  'start': 11,\n","  'end': 17},\n"," {'entity_group': 'ORG',\n","  'score': 0.9897879,\n","  'word': 'The University of Michigan',\n","  'start': 32,\n","  'end': 58},\n"," {'entity_group': 'LOC',\n","  'score': 0.9991241,\n","  'word': 'Michigan',\n","  'start': 62,\n","  'end': 70},\n"," {'entity_group': 'LOC',\n","  'score': 0.9989122,\n","  'word': 'USA',\n","  'start': 72,\n","  'end': 75}]"]},"metadata":{},"execution_count":47}]},{"cell_type":"markdown","source":["### *Try it yourself!*"],"metadata":{"id":"bZFv40JVnep8"}},{"cell_type":"code","source":["#classify some text\n","ner(\"YOUR TEXT HERE\")"],"metadata":{"id":"smCN9JBnniC1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## *Task 5: Generate text*\n","\n","Text generation is a subfield of Natural Language Processing (NLP) that focuses on creating human-like text using machine learning models. The goal of text generation is to generate coherent and contextually relevant sentences, paragraphs, or longer pieces of text that resemble human-written content.\n","\n","Text generation is an exciting area in NLP, and advancements in deep learning and language models have led to impressive results in various applications. As the field continues to evolve, text generation models are expected to become even more sophisticated and capable of generating high-quality and contextually relevant text."],"metadata":{"id":"csyFOVmka_5-"}},{"cell_type":"code","source":["#load in the generation model\n","generator = pipeline(\"text-generation\",\n","                     model=\"distilgpt2\") #note that here we are using gpt2 as it is better suited for text generation\n","\n","#generate some text\n","generator(\n","    \"Climate change is caused by\",\n","    max_length=100,\n","    num_return_sequences=2,\n",")"],"metadata":{"id":"RJHyQizFbGTk","executionInfo":{"status":"ok","timestamp":1691516253815,"user_tz":240,"elapsed":9799,"user":{"displayName":"Ken Reid","userId":"08628755971426398220"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"de2f43ae-21a5-4e2a-85ac-4fcef7a5f0c9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'generated_text': 'Climate change is caused by other factors in the atmosphere.\\n\\n\\n\\nSo if you are looking for information on how our climate will affect your global carbon level, look this up:\\nClimate change is caused by other factors in our atmosphere. Climate change is caused by other factorsveying the atmosphere as a result of other factors, and these causes are most common in regions where precipitation and surface temperatures are higher.\\nSo by definition, the only cause of the increase in cloud cover is air-'},\n"," {'generated_text': 'Climate change is caused by a large decrease in levels of fossil fuel concentrations. These results may contribute to an important increase in surface temperature and therefore temperature loss in the tropics. The increase in human activity could be due to climate change.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe following graphs show the rate of surface temperature and global warming across the United States and the United Kingdom, and are based on the'}]"]},"metadata":{},"execution_count":48}]},{"cell_type":"markdown","source":["### *Try it yourself!*"],"metadata":{"id":"XDmKWMRwmNvy"}},{"cell_type":"code","source":["#generate some text\n","generator(\n","    \"YOUR TEXT HERE\",\n","    max_length=\"YOUR MAX LENGTH HERE\",\n","    num_return_sequences=\"YOUR NUMBER OF RETURNS HERE\",\n",")"],"metadata":{"id":"50V5tsMrmP89"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## *Task 6: Comparing sentences*\n","\n","Comparing sentences in Natural Language Processing (NLP) involves assessing the similarity or dissimilarity between two or more sentences. This task is essential for various applications, such as duplicate detection, plagiarism detection, paraphrase identification, and more. There are different methods and techniques used in NLP to compare sentences, each with its advantages and limitations.\n","\n","The choice of the most appropriate method for comparing sentences depends on the specific application and the nature of the data. Some methods are better suited for capturing semantic similarity, while others are more suitable for capturing surface-level similarity or paraphrases. It is essential to evaluate and select the method that best fits the requirements of the particular NLP task at hand. Today we will implement BERTSimilarity."],"metadata":{"id":"Zqie-4hfyFJ7"}},{"cell_type":"code","source":["%%capture\n","!pip install BERTSimilarity"],"metadata":{"id":"HNK88683yIiR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import BERTSimilarity.BERTSimilarity as bertsimilarity"],"metadata":{"id":"GyXeQkndyEfB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["f1='The man is playing soccer.'\n","f2='The man is playing football.'\n","bertsimilarity=bertsimilarity.BERTSimilarity()\n","dist=bertsimilarity.calculate_distance(f1,f2)\n","print('The distance between sentence1: '+f1+' and sentence2: '+f2+' is '+str(dist))\n"],"metadata":{"id":"QfeNGA6ayQzf","executionInfo":{"status":"ok","timestamp":1691516262998,"user_tz":240,"elapsed":3933,"user":{"displayName":"Ken Reid","userId":"08628755971426398220"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"54217953-ea7f-46d8-9f71-c7fba013c817"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The distance between sentence1: The man is playing soccer. and sentence2: The man is playing football. is 0.9718283414840698\n"]}]},{"cell_type":"code","source":["bertsimilarity=bertsimilarity.BERTSimilarity()\n","\n","\n","f1='Tiger conservation needs more funding'\n","f2='Renewable energy should replace fossil fuels by 2030'\n","f3='The Titanic sank in the Atlantic Ocean'\n","f4='The Pacific Ocean is deeper in the Atlantic Ocean'\n","d1=bertsimilarity.calculate_distance(f1,f2)\n","d2=bertsimilarity.calculate_distance(f1,f3)\n","d3=bertsimilarity.calculate_distance(f3,f4)\n","d4=bertsimilarity.calculate_distance(f1,f4)\n","print('The distance between sentence 1: '+f1+' and sentence 2: '+f2+' is '+str(d1))\n","print('The distance between sentence 1: '+f1+' and sentence 3: '+f3+' is '+str(d2))\n","print('The distance between sentence 3: '+f3+' and sentence 4: '+f4+' is '+str(d3))\n","print('The distance between sentence 1: '+f1+' and sentence 4: '+f4+' is '+str(d4))"],"metadata":{"id":"6ZER4h48ydV5","executionInfo":{"status":"error","timestamp":1691516263115,"user_tz":240,"elapsed":121,"user":{"displayName":"Ken Reid","userId":"08628755971426398220"}},"colab":{"base_uri":"https://localhost:8080/","height":245},"outputId":"9e344e01-cba5-404f-fa73-3cd9aac79b3b"},"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-52-5e98b188b330>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbertsimilarity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbertsimilarity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBERTSimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Tiger conservation needs more funding'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Renewable energy should replace fossil fuels by 2030'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'BERTSimilarity' object has no attribute 'BERTSimilarity'"]}]},{"cell_type":"markdown","source":["## *Task 7. Text Summarization*.\n","\n","Text summarization aims to create concise summaries of longer documents while retaining the essential information. This could be useful for distilling information from large documents, or summarize papers. The goal of text summarization is to provide a coherent and meaningful summary that captures the key ideas and main concepts present in the original text.\n","\n","Text summarization has a wide variety of applications:\n","* News Summarization: Condensing lengthy news articles into shorter summaries for quick and efficient consumption.\n","* Document Summarization: Creating concise summaries of research papers, reports, or legal documents for better comprehension.\n","* Social Media Summarization: Generating summaries of long social media threads or conversations for better readability."],"metadata":{"id":"y5j-MkDDtE7D"}},{"cell_type":"code","source":["summarizer = pipeline(\"summarization\", model=\"t5-base\", tokenizer=\"t5-base\", framework=\"tf\")"],"metadata":{"id":"Yi5XZCcWxd9r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rio_info  = r\"\"\"\n","The Rio Earth Summit, also known as the United Nations Conference on Environment and Development (UNCED), was a significant international event held in Rio de Janeiro, Brazil, from June 3 to June 14, 1992. It brought together representatives from 172 governments, along with numerous non-governmental organizations, businesses, and other stakeholders. The summit's primary purpose was to address global environmental and developmental challenges and promote sustainable development worldwide.\n","\n","During the Rio Earth Summit, several key outcomes were achieved:\n","\n","Agenda 21: The conference led to the adoption of Agenda 21, an extensive action plan outlining sustainable development strategies in various areas, including poverty reduction, public health, biodiversity conservation, natural resource management, and climate change mitigation.\n","\n","Climate Change: The United Nations Framework Convention on Climate Change (UNFCCC) was established as a result of the summit. The UNFCCC aimed to address the issue of climate change on a global scale by encouraging international cooperation and action.\n","\n","Biodiversity: The Convention on Biological Diversity (CBD) was another significant outcome of the summit. The CBD focused on the conservation and sustainable use of biodiversity, as well as equitable sharing of benefits from genetic resources.\n","\n","Forest Principles: The summit also led to the adoption of the Non-Legally Binding Authoritative Statement of Principles for a Global Consensus on the Management, Conservation, and Sustainable Development of All Types of Forests, commonly known as the \"Forest Principles.\" These principles provided guidance on sustainable forest management.\n","\n","Rio Declaration: The Declaration on Environment and Development, also known as the Rio Declaration, presented 27 principles that emphasized the importance of integrating environmental protection and social equity in sustainable development efforts.\n","\n","Commission on Sustainable Development (CSD): The Rio Earth Summit established the Commission on Sustainable Development to monitor and promote the implementation of Agenda 21 and other sustainable development initiatives.\n","\n","Overall, the Rio Earth Summit played a pivotal role in raising global awareness about the need for sustainable development and the interconnections between environmental, social, and economic issues. Although it did not impose binding commitments, the summit laid the groundwork for subsequent international agreements and actions aimed at addressing environmental challenges and promoting sustainable development worldwide.\n","\"\"\""],"metadata":{"id":"Vgf9b1tWvNY3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summary=summarizer(rio_info, max_length=130, min_length=60)\n","print(summary)"],"metadata":{"id":"aYzt2cVeuZXm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## *Task 8. Question Answering*\n","\n","Question answering (QA) in Natural Language Processing (NLP) is a task that involves designing systems capable of understanding natural language questions and providing accurate and relevant answers from a given set of documents or knowledge sources. The goal of QA systems is to emulate human-like comprehension and reasoning to respond to user queries in a precise and contextually relevant manner.\n","\n","Question answering has both comercial and research applications:\n","* Virtual Assistants: Virtual assistants like Siri, Alexa, and Google Assistant use QA systems to answer user queries on a wide range of topics.\n","* Customer Support: QA systems are used in chatbots and customer support platforms to provide quick and accurate answers to customer queries.\n","* Educational Platforms: QA systems are employed in educational platforms to answer student questions and provide explanations.\n","* Medical Diagnosis: QA systems can assist healthcare professionals in accessing medical knowledge and providing accurate diagnoses.\n","* Information Retrieval: QA systems are used to extract specific information from large databases or textual documents."],"metadata":{"id":"CK_iR0vBwMp4"}},{"cell_type":"code","source":["questioner = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")"],"metadata":{"id":"lyz2jN2Ix8yz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["kyoto_info = r\"\"\"\n","The Kyoto Protocol is an international treaty aimed at addressing the issue of global climate change by reducing greenhouse gas emissions. It was adopted on December 11, 1997, in Kyoto, Japan, during the United Nations Framework Convention on Climate Change (UNFCCC) Conference of Parties (COP 3). The treaty entered into force on February 16, 2005, after it was ratified by enough countries to account for at least 55% of the total carbon dioxide emissions from 1990 among the parties to the UNFCCC.\n","\n","Main objectives of the Kyoto Protocol:\n","\n","Emission Reduction Targets: The protocol established legally binding emission reduction targets for developed countries, also known as Annex I countries. These countries committed to reducing their greenhouse gas emissions collectively by an average of 5.2% below 1990 levels during the first commitment period from 2008 to 2012.\n","\n","Flexible Mechanisms: To help countries meet their emission reduction targets cost-effectively, the Kyoto Protocol introduced three flexible mechanisms:\n","a. Emissions Trading: Allowed countries to buy and sell emission allowances among themselves.\n","b. Clean Development Mechanism (CDM): Enabled developed countries to invest in emission reduction projects in developing countries and receive credits for the reductions achieved.\n","c. Joint Implementation (JI): Permitted developed countries to collaborate on emission reduction projects in other developed countries and receive credits.\n","\n","Adaptation Fund: The protocol established the Adaptation Fund to provide financial assistance to developing countries to help them cope with the impacts of climate change.\n","\n","Review and Compliance: The Kyoto Protocol implemented a rigorous system to review and assess countries' progress towards meeting their emission reduction targets and ensuring compliance with the treaty's provisions.\n","\n","The Kyoto Protocol was a significant milestone in international efforts to combat climate change. However, it also faced challenges, particularly due to the absence of emission reduction commitments for major developing countries, such as China and India. As a result, the protocol was not able to address the increasing emissions from these countries and was considered by some as insufficient to fully tackle the global climate crisis.\n","\n","In the years following its adoption, international climate negotiations continued, leading to the development of the Paris Agreement in 2015. The Paris Agreement built upon the principles of the Kyoto Protocol but introduced a more inclusive and flexible approach, with countries setting their own voluntary emission reduction targets (Nationally Determined Contributions) to limit global warming well below 2 degrees Celsius above pre-industrial levels.\n","\"\"\""],"metadata":{"id":"l_X3bdV4xWeo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = questioner(question=\"When was the Kyoto Protocol adopted?\", context=kyoto_info)\n","print(result['answer'])"],"metadata":{"id":"bYv48FCttEL9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = questioner(question=\"What agreemnt was made in 2015?\", context=kyoto_info)\n","print(result['answer'])"],"metadata":{"id":"IQuwxsMqxyuM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## *Task 9. Translation*\n","\n","NLP translation refers to the application of Natural Language Processing (NLP) techniques to automatically translate text from one language to another. The goal of NLP translation is to enable computers to understand the meaning of sentences in the source language and produce equivalent sentences in the target language, preserving the context and semantics as much as possible.\n","\n","* Language Translation Services: NLP translation is widely used in online language translation services like Google Translate, Microsoft Translator, and others, enabling users to translate text between multiple languages in real-time.\n","* Multilingual Content Creation: NLP translation allows content creators to efficiently produce content in multiple languages, making their work accessible to global audiences.\n","* International Communication: NLP translation facilitates communication and collaboration across language barriers in various domains, including business, research, and diplomacy.\n","* Localization of Software and Apps: NLP translation is used to adapt software, websites, and mobile apps to different languages and cultures, making them more user-friendly for diverse audiences.\n","\n","NLP translation is a complex and challenging task that requires deep understanding of language structures and context. Advances in deep learning, particularly in transformer-based models like BERT and GPT, have significantly improved the quality and fluency of machine translation systems, making them valuable tools for cross-lingual communication and information exchange, however you should still carefully choose the correct model for your ranslation task. Here, we will look at a few models:\n","* English to french model: https://huggingface.co/Helsinki-NLP/opus-mt-en-fr\n","* English to spanish model: https://huggingface.co/Helsinki-NLP/opus-mt-tc-big-en-es"],"metadata":{"id":"yRfQ2DZQ0N8b"}},{"cell_type":"code","source":["!pip install sentencepiece"],"metadata":{"id":"RVLC_H-o04_C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Restart the Notebook\n","After installing, some packages that were already loaded were updated and in order to correctly use them, we should now restart the notebook.\n","\n","From the Menu:\n","\n","Runtime → Restart Runtime"],"metadata":{"id":"szDWIFa-6q0J"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline"],"metadata":{"id":"oSFmQStn2pyF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")"],"metadata":{"id":"D63tyxRN0Unz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["translator = pipeline(\"translation\", model = model, tokenizer = tokenizer)\n","\n","translated = translator('Climate change is a global issue that needs to be effectively communicated to people around the world.')[0].get('translation_text')\n","print(translated)"],"metadata":{"id":"GWhpaKis0qG0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### *Try it yourself!*"],"metadata":{"id":"U3hGPLp_n5BC"}},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(\"YOUR CHOSEN MODEL HERE\")\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"YOUR CHOSEN MODEL HERE\")\n","\n","translator = pipeline(\"translation\", model = model, tokenizer = tokenizer)\n","\n","translated = translator('YOUR TEXT TO TRANSLATE HERE')[0].get('translation_text')\n","print(translated)"],"metadata":{"id":"c8uNsJIAn5ye"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## *Task 10. Identifying toxic comments*\n","\n","Identifying toxic comments is a Natural Language Processing (NLP) task that involves detecting offensive, harmful, or inappropriate language in text, typically in online platforms, forums, or social media. The goal is to automatically identify and filter out comments that contain offensive or toxic content to maintain a safer and more respectful online environment. To do this we will use the detoxify model: https://github.com/unitaryai/detoxify."],"metadata":{"id":"CHmpw8i24Ino"}},{"cell_type":"code","source":["%%capture\n","! pip install detoxify\n","! pip install pytorch-transformer"],"metadata":{"id":"ZhTcoxjj4EO3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Restart the Notebook\n","After installing, some packages that were already loaded were updated and in order to correctly use them, we should now restart the notebook.\n","\n","From the Menu:\n","\n","Runtime → Restart Runtime"],"metadata":{"id":"GXlsUpBS6o-z"}},{"cell_type":"code","source":["from detoxify import Detoxify\n","import pandas as pd\n","\n","# each model takes in either a string or a list of strings\n","results = Detoxify('unbiased').predict(['The mayor is an idiot they should spend money on things that matter not climate change',\n","                                        'So greatful for all the work greenpeace does to promote a sustainable future'])\n","\n","print(pd.DataFrame(results))"],"metadata":{"id":"x55Qxalt4XYo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### *Try it yourself!*"],"metadata":{"id":"cLeUM-pVoV7Q"}},{"cell_type":"code","source":["results = Detoxify('unbiased').predict(['YOUR TEXT HERE'])\n","\n","print(pd.DataFrame(results))"],"metadata":{"id":"ZM0r6mczoWf-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#**Section 2. Sentiment analysis of large text databases using pre-trained models**\n","\n","In this session, we will focus on harnessing the power of pre-trained models to analyze sentiments in massive text databases, specifically exploring public perception towards hunting using tweets.\n","\n","Sentiment analysis, also known as opinion mining, is a prominent Natural Language Processing (NLP) task that aims to understand and categorize the emotional tone expressed in text. By leveraging cutting-edge pre-trained models like BERT, GPT-3, or similar transformer-based architectures, we can efficiently analyze sentiments at scale, uncovering valuable insights from vast amounts of textual data.\n","\n","In this workshop, we will guide you through the process of conducting sentiment analysis on a large dataset of tweets related to hunting. We will cover:\n","\n","* Data Collection and Preprocessing: We will walk you through the steps of collecting tweets related to hunting and preparing the data for sentiment analysis.\n","* Utilizing Pre-trained Models: You will learn how to access and apply pre-trained models for sentiment analysis, capitalizing on their capabilities to comprehend the context and nuances of language.\n","* Sentiment Classification: We will demonstrate how to classify tweets into positive, negative, or neutral sentiments, providing a comprehensive understanding of public sentiment towards hunting.\n","* Visualization and Insights: Discover how to visualize sentiment and gain meaningful insights from the sentiment analysis results.\n","* Ethical Considerations: As we delve into sensitive topics like hunting, we will discuss ethical considerations and biases that may arise during sentiment analysis.\n","\n","By the end of this section, you will have hands-on experience in conducting sentiment analysis on large text databases using pre-trained models. Additionally, you will have the tools to apply these techniques to other domains and explore public perceptions on various topics through social media or other datasets.\n","\n","*Disclaimer: to comply with the Twitter API developer policy and to maintain high ethical standards, these are not real Tweets. These are fictional Tweets generated by ChatGPT 3.5.\"*\n"],"metadata":{"id":"V0l0t-8knqms"}},{"cell_type":"code","source":["# install the the needed libraries\n","%%capture\n","! pip install -U accelerate\n","! pip install -U transformers"],"metadata":{"id":"NTcU9QuaU_Iv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Restart the Notebook\n","After installing, some packages that were already loaded were updated and in order to correctly use them, we should now restart the notebook.\n","\n","From the Menu:\n","\n","Runtime → Restart Runtime"],"metadata":{"id":"sd6___Ez6nVw"}},{"cell_type":"code","source":["# import required packages\n","import torch\n","import pandas as pd\n","import numpy as np\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n","\n","# Create class for data preparation\n","class SimpleDataset:\n","    def __init__(self, tokenized_texts):\n","        self.tokenized_texts = tokenized_texts\n","\n","    def __len__(self):\n","        return len(self.tokenized_texts[\"input_ids\"])\n","\n","    def __getitem__(self, idx):\n","        return {k: v[idx] for k, v in self.tokenized_texts.items()}"],"metadata":{"id":"rVQorwYtVPwr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load tokenizer and model, create trainer\n","model_name = \"j-hartmann/emotion-english-distilroberta-base\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","trainer = Trainer(model=model)"],"metadata":{"id":"35DHfS4bV2UX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## *Upload text data*"],"metadata":{"id":"HUsmuDBOV9ek"}},{"cell_type":"code","source":["import pandas as pd\n","\n","url = 'https://raw.githubusercontent.com/Dr-Nathan-Fox/AI_Workshop/main/hunting_tweets.csv'\n","df_pred = pd.read_csv(url)\n","\n","df_pred.head()"],"metadata":{"id":"D8_c9V2sWB4E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert text column to a list of texts\n","text_column = \"text\"  # select the column in your csv that contains the text to be classified\n","pred_texts = df_pred[text_column].dropna().astype('str').tolist()"],"metadata":{"id":"zlAHnej4V45f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## *Classify texts with model*"],"metadata":{"id":"x8FlLI97WGcf"}},{"cell_type":"code","source":["# Tokenize texts and create prediction data set\n","tokenized_texts = tokenizer(pred_texts,truncation=True,padding=True)\n","pred_dataset = SimpleDataset(tokenized_texts)"],"metadata":{"id":"cHNgpcIrWI1M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Run predictions\n","predictions = trainer.predict(pred_dataset)"],"metadata":{"id":"AT7F_S4yWKd1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Transform predictions to labels\n","preds = predictions.predictions.argmax(-1)\n","labels = pd.Series(preds).map(model.config.id2label)\n","scores = (np.exp(predictions[0])/np.exp(predictions[0]).sum(-1,keepdims=True)).max(1)"],"metadata":{"id":"Ev4WlvjNWMU6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels\n","\n"],"metadata":{"id":"1GVxjuNzZ73R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# scores raw\n","temp = (np.exp(predictions[0])/np.exp(predictions[0]).sum(-1,keepdims=True))"],"metadata":{"id":"bg0dIzznWNnS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Join the predections back to the original\n","# container\n","anger = []\n","disgust = []\n","fear = []\n","joy = []\n","neutral = []\n","sadness = []\n","surprise = []\n","\n","# extract scores (as many entries as exist in pred_texts)\n","for i in range(len(pred_texts)):\n","  anger.append(temp[i][0])\n","  disgust.append(temp[i][1])\n","  fear.append(temp[i][2])\n","  joy.append(temp[i][3])\n","  neutral.append(temp[i][4])\n","  sadness.append(temp[i][5])\n","  surprise.append(temp[i][6])"],"metadata":{"id":"8lu-2t6dWPZi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create DataFrame with texts, predictions, labels, and scores\n","df = pd.DataFrame(list(zip(pred_texts,preds,labels, scores,  anger, disgust, fear, joy, neutral, sadness, surprise)), columns=['text','pred','label','score', 'anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise'])\n","df.head()"],"metadata":{"id":"VHvh-ab0WQoD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## *Create a plot of the frequencies of each sentiment group*"],"metadata":{"id":"04I5FnixbWQ9"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Count the frequency of each category in the 'category' column\n","category_counts = df['label'].value_counts()\n","\n","# Create a bar chart\n","plt.figure(figsize=(8, 6))\n","plt.bar(category_counts.index, category_counts.values)\n","plt.xlabel('Categories')\n","plt.ylabel('Frequency')\n","plt.title('Frequency of Categories')\n","plt.show()"],"metadata":{"id":"eorLXqJ0aY5B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## *Exctract Tweets associated with a specific sentiment*"],"metadata":{"id":"JEGyqx94bLrc"}},{"cell_type":"code","source":["#return the tweets listed as anger\n","anger_df = df[df.label == \"anger\"]\n","\n","anger_df.text"],"metadata":{"id":"ih-uK33za32M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## *Export results*"],"metadata":{"id":"Hjfg800JWSnc"}},{"cell_type":"code","source":["# save results to csv\n","YOUR_FILENAME = \"hunting_sentiment.csv\"  # name your output file\n","df.to_csv(YOUR_FILENAME)"],"metadata":{"id":"H60v5-mnWVC1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# download file\n","files.download(YOUR_FILENAME)"],"metadata":{"id":"uVvKj8A8WWHS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#**Section 3. Topic modeling of large text databases using pre-trained models**\n","\n","In this section, we will learn how to apply topic modeling and explore what people talk about in reference to climate change when tweeting.\n","\n","Topic modeling is a powerful technique in Natural Language Processing (NLP) that allows us to uncover hidden themes or topics present in a large collection of text data. By combining the capabilities of pre-trained models, such as BERT, GPT-3, or similar transformer-based architectures, we can efficiently analyze massive text databases and gain valuable insights into the discussions surrounding climate change on social media.\n","\n","* Throughout this section section, we will guide you through the process of topic modeling on a vast dataset of tweets related to climate change. We will cover the following key aspects:\n","* Data Collection and Preprocessing: We will walk you through the steps of collecting tweets related to climate change and preparing the data for topic modeling.\n","* Utilizing Pre-trained Models: You will learn how to access and employ pre-trained models to understand the contextual representation of words and phrases, essential for topic modeling.\n","* Topic Extraction: We will demonstrate how to extract meaningful topics from the tweets using state-of-the-art topic modeling techniques.\n","* Topic Visualization and Interpretation: Discover how to visualize the identified topics and interpret the underlying themes in the context of climate change discussions.\n","* Insights and Applications: Uncover valuable insights into the prevailing topics and trends surrounding climate change on social media and explore potential real-world applications.\n","* Ethical Considerations: As we explore sensitive topics like climate change, we will discuss ethical considerations and potential biases that may arise during topic modeling.\n","\n","By the end of this section, you will have practical experience in conducting topic modeling on large text databases using pre-trained models. Additionally, you will gain the skills to apply these techniques to other domains, empowering you to discover hidden patterns and themes in vast amounts of textual data.\n","\n","*Disclaimer: to comply with the Twitter API developer policy and to maintain high ethical standards, these are not real Tweets. These are fictional Tweets generated by ChatGPT 3.5.\"*\n"],"metadata":{"id":"RUtagJkenxJG"}},{"cell_type":"code","source":["# install the the needed libraries\n","%%capture\n","!pip install -U numpy==1.11.0.\n","!pip install -U bertopic"],"metadata":{"id":"RI4QshVWt4UP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Restart the Notebook\n","After installing, some packages that were already loaded were updated and in order to correctly use them, we should now restart the notebook.\n","\n","From the Menu:\n","\n","Runtime → Restart Runtime"],"metadata":{"id":"PoQYyHz8t9wf"}},{"cell_type":"markdown","source":["## *Data*\n","For this example, we use the"],"metadata":{"id":"lAsyTzuwuOhV"}},{"cell_type":"code","source":["import pandas as pd\n","\n","url = 'https://raw.githubusercontent.com/Dr-Nathan-Fox/AI_Workshop/main/climate_tweets.csv'\n","df_pred = pd.read_csv(url)\n","\n","df_pred.head()"],"metadata":{"id":"qGr-QE9xezum"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## *Preprocess the text*"],"metadata":{"id":"vsBFI9SFuiNQ"}},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')\n","nltk.download('omw-1.4')\n","nltk.download('wordnet')\n","wn = nltk.WordNetLemmatizer()\n","\n","stopwords = nltk.corpus.stopwords.words('english')\n","\n","# Remove stopwords\n","df_pred['text_without_stopwords'] = df_pred['text'].apply(lambda x: ' '.join([w for w in x.split() if w.lower() not in stopwords]))\n","\n","# Lemmatization\n","df_pred['text_lemmatized'] = df_pred['text_without_stopwords'].apply(lambda x: ' '.join([wn.lemmatize(w) for w in x.split() if w not in stopwords]))\n","\n","# Take a look at the data\n","df_pred.head()"],"metadata":{"id":"Ls6eZ3BPun8F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Save text as a list for topic modeling"],"metadata":{"id":"_Pzq8fxUu06C"}},{"cell_type":"code","source":["docs = list(df_pred['text_lemmatized'])"],"metadata":{"id":"XG1mMDNzu7_A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##*Topic Modeling*\n","\n","We start by instantiating BERTopic. We set language to `english` since our documents are in the English language. If you would like to use a multi-lingual model, please use `language=\"multilingual\"` instead.\n","\n","We will also calculate the topic probabilities. However, this can slow down BERTopic significantly at large amounts of data (>100_000 documents). It is advised to turn this off if you want to speed up the model."],"metadata":{"id":"oMPeb9SO2td2"}},{"cell_type":"code","source":["from bertopic import BERTopic\n","\n","topic_model = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True)\n","topics, probs = topic_model.fit_transform(docs)"],"metadata":{"id":"jYhUis0B25fF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**NOTE**: Use `language=\"multilingual\"` to select a model that support 50+ languages."],"metadata":{"id":"FTpE4-iO6eW6"}},{"cell_type":"markdown","source":["## Attributes\n","\n","There are a number of attributes that you can access after having trained your BERTopic model:\n","\n","\n","| Attribute | Description |\n","|------------------------|---------------------------------------------------------------------------------------------|\n","| topics_               | The topics that are generated for each document after training or updating the topic model. |\n","| probabilities_ | The probabilities that are generated for each document if HDBSCAN is used. |\n","| topic_sizes_           | The size of each topic                                                                      |\n","| topic_mapper_          | A class for tracking topics and their mappings anytime they are merged/reduced.             |\n","| topic_representations_ | The top *n* terms per topic and their respective c-TF-IDF values.                             |\n","| c_tf_idf_              | The topic-term matrix as calculated through c-TF-IDF.                                       |\n","| topic_labels_          | The default labels for each topic.                                                          |\n","| custom_labels_         | Custom labels for each topic as generated through `.set_topic_labels`.                                                               |\n","| topic_embeddings_      | The embeddings for each topic if `embedding_model` was used.                                                              |\n","| representative_docs_   | The representative documents for each topic if HDBSCAN is used.                                                |\n","\n","For example, to access the predicted topics for the first 10 documents, we simply run the following:"],"metadata":{"id":"TxXwase_6_y9"}},{"cell_type":"code","source":["topic_model.topics_[:10]"],"metadata":{"id":"fHClUwjJ67Sf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## *Extracting Topics*\n","After fitting our model, we can start by looking at the results. Typically, we look at the most frequent topics first as they best represent the collection of documents."],"metadata":{"id":"luQXTTNM6p5u"}},{"cell_type":"code","source":["freq = topic_model.get_topic_info(); freq#.head(5)\n","\n","#save topics as .csv\n","freq.to_csv('freq.csv', index=False)\n","freq"],"metadata":{"id":"Uxia3TVN6sj0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["-1 refers to all outliers and should typically be ignored. Next, let's take a look at a frequent topic that were generated:"],"metadata":{"id":"wYU4chgZ6xuq"}},{"cell_type":"code","source":["topic_model.get_topic(0)  # Select the most frequent topic"],"metadata":{"id":"oDNRknJ36yxq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## *Saving topics data*\n","\n","If we are interested in saving a list of which documents are associated with which topic, we can do this by creating a dataframe contianing both documents and topics. First lets print out the top ten documents associate with the most frequent topic."],"metadata":{"id":"6CTQuX8XNshm"}},{"cell_type":"code","source":["#create a dataframe with each document and its topic number\n","df = pd.DataFrame({'topic': topics, 'document': docs})\n","\n","#return the top ten documents from a topic (here topic 0)\n","topic_0 = df[df.topic == 0][:10] #get first 10 documents in a topic\n","\n","#print as a list\n","topic_0['document'].tolist()"],"metadata":{"id":"pLIZ7kL-_DxB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, lets say we are super interested in topic 4 and want to extract all documents for that individual topic, we can repeate the above and then save the output dataframe as a .csv."],"metadata":{"id":"58l3he3VRrgn"}},{"cell_type":"code","source":["#get all documents belonging to a specific topic (here topic 4)\n","topic_4 = df[df.topic == 4][:len(df[df.topic == 4])] # get all documents in a topic\n","\n","#save a .csv\n","topic_4.to_csv('topic_4.csv', index=False)\n"],"metadata":{"id":"RpaLmyS2L40v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can also print the top documents for each topic"],"metadata":{"id":"OQi2eIVvQXOO"}},{"cell_type":"code","source":["for topic in range(10):\n","    topic_info = topic_model.get_topic(topic)\n","    representative_docs = df[df.topic == topic][:10]['document'].tolist()\n","\n","    print(\"Topic: \", topic)\n","    print(\"Topic Information: \")\n","    print(topic_info)\n","    print(\"Representative Documents: \")\n","    print(representative_docs)\n","    print(\"\\n\")"],"metadata":{"id":"x3NpSrnx-6B_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can save these same results as a .txt"],"metadata":{"id":"s4mwY6BNQfha"}},{"cell_type":"code","source":["\n","with open('topic_info_and_representative_docs.txt', 'w') as f:\n","    # Loop over the topics and write the output to the file\n","    for topic in range(10):\n","        topic_info = topic_model.get_topic(topic)\n","        representative_docs = df[df.topic == topic][:10]['document'].tolist()\n","\n","        f.write(f\"Topic: {topic}\\n\")\n","        f.write(\"Topic Information: \\n\")\n","        f.write(f\"{topic_info}\\n\")\n","        f.write(\"Representative Documents: \\n\")\n","        for doc in representative_docs:\n","            f.write(f\"{doc}\\n\")\n","        f.write(\"\\n\")"],"metadata":{"id":"j6L0Ywyx_ZaR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## *Searching for topics related to your work*"],"metadata":{"id":"D7NDLegSxMjU"}},{"cell_type":"code","source":["similar_topics, similarity = topic_model.find_topics(\"biodiveristy\", top_n=5); similar_topics"],"metadata":{"id":"juvLlb55xQ5M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["topic_model.get_topic(71) #update the topic to one that matches"],"metadata":{"id":"wggvkBjyxgCl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## *Visualization*\n","There are several visualization options available in BERTopic, namely the visualization of topics, probabilities and topics over time. Topic modeling is, to a certain extent, quite subjective. Visualizations help understand the topics that were created."],"metadata":{"id":"HJZqaYuK7F1f"}},{"cell_type":"code","source":["topic_model.visualize_barchart(top_n_topics=10)"],"metadata":{"id":"u69wGbIP9nQ4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save the figure\n","fig = topic_model.visualize_barchart(top_n_topics=10)\n","fig.write_html(\"climate_topics.html\")"],"metadata":{"id":"U-M_Oq2NM8ZQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Visualize Topic Hierarchy\n","\n","The topics that were created can be hierarchically reduced. In order to understand the potential hierarchical structure of the topics, we can use scipy.cluster.hierarchy to create clusters and visualize how they relate to one another. This might help selecting an appropriate nr_topics when reducing the number of topics that you have created."],"metadata":{"id":"Ttt_3oTOdO9T"}},{"cell_type":"code","source":["topic_model.visualize_hierarchy(top_n_topics=10)"],"metadata":{"id":"evXDuF2GdQN2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Visualize Topics\n","After having trained our `BERTopic` model, we can iteratively go through perhaps a hundred topic to get a good\n","understanding of the topics that were extract. However, that takes quite some time and lacks a global representation."],"metadata":{"id":"-Co2SQ1gc46L"}},{"cell_type":"code","source":["topic_model.visualize_topics()"],"metadata":{"id":"XE-d7SyCc95a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Visualize Topic Similarity\n","Having generated topic embeddings, through both c-TF-IDF and embeddings, we can create a similarity matrix by simply applying cosine similarities through those topic embeddings. The result will be a matrix indicating how similar certain topics are to each other."],"metadata":{"id":"S440oWMnTEdY"}},{"cell_type":"code","metadata":{"id":"edzNhZuZ6wTr"},"source":["topic_model.visualize_heatmap(n_clusters=8, width=1000, height=1000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save the figure\n","fig = topic_model.visualize_heatmap(n_clusters=4, width=1000, height=1000, topics = [0,1,2,3,4])\n","fig.write_html(\"climate_heat.html\")"],"metadata":{"id":"fcTCpZNJUxek"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Visualize Term Score Decline\n","Topics are represented by a number of words starting with the best representative word. Each word is represented by a c-TF-IDF score. The higher the score, the more representative a word to the topic is. Since the topic words are sorted by their c-TF-IDF score, the scores slowly decline with each word that is added. At some point adding words to the topic representation only marginally increases the total c-TF-IDF score and would not be beneficial for its representation.\n","\n","To visualize this effect, we can plot the c-TF-IDF scores for each topic by the term rank of each word. In other words, the position of the words (term rank), where the words with the highest c-TF-IDF score will have a rank of 1, will be put on the x-axis. Whereas the y-axis will be populated by the c-TF-IDF scores. The result is a visualization that shows you the decline of c-TF-IDF score when adding words to the topic representation. It allows you, using the elbow method, the select the best number of words in a topic."],"metadata":{"id":"SZ5qk8BKeJ1B"}},{"cell_type":"code","source":["topic_model.visualize_term_rank()"],"metadata":{"id":"apREdT20eI_L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Section 4. Applying these methods to your own work**\n","\n","In this section, I will equip you with valuable tools and techniques to apply the methods we've explored in the previous sections to your own NLP projects effectively.\n","\n","Every NLP project is unique, and you may have specific requirements or datasets to work with. To assist you in your journey, I will provide practical code snippets and tips that you can directly integrate into your projects. Whether you are a beginner or an experienced NLP practitioner, this section is designed to enhance your productivity and boost your confidence in implementing NLP tasks using pre-trained models.\n","\n","By the end of this section, you will be well-equipped to take the knowledge gained in this workshop and apply it confidently to your own NLP projects. Whether you are working on sentiment analysis, topic modeling, or any other NLP task, these code snippets and tips will serve as valuable resources to make your work more efficient and effective."],"metadata":{"id":"bvTDMoftbpqm"}},{"cell_type":"markdown","source":["## *Uploading your own data from your personal folders.*\n","\n","Today the data we loaded was stored in a raw format on GitHub. The data was stored there to allow for simplicity and reproducibiltiy for all users at todays workshop. However, it is unlikely that your data will be stored in this format and it may be time consuming for you to transfer it to GitHub. A simple method for gettting your data onto Google Colab is directly uploading your data - here I will show you how."],"metadata":{"id":"c9iUbC8wbw-T"}},{"cell_type":"code","source":["# run cell and select file for upload\n","from google.colab import files\n","files.upload()"],"metadata":{"id":"xxgJgJTcckYe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# specify your filename\n","file_name = \"/content/YOUR_FILENAME.csv\"  # note: you can right-click on your file and copy-paste the path to it here\n","text_column = \"text\"  # select the column in your csv that contains the text to be classified\n","\n","# read in csv\n","df_pred = pd.read_csv(file_name)\n","pred_texts = df_pred[text_column].dropna().astype('str').tolist()"],"metadata":{"id":"KBoYxf6Kcnx5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## *Citations and additional resources*\n","\n","*This section of the workshop has been designed and adapted previous implementations of BERT. For more information, check out:*\n","\n","* *https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb*\n","*   *https://maartengr.github.io/BERTopic/index.html*\n","*   *https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing#scrollTo=AXHLDxJdRzBi*\n","*   *https://huggingface.co/docs/transformers/model_doc/bert*\n","*   *https://colab.research.google.com/github/j-hartmann/emotion-english-distilroberta-base/blob/main/emotion_prediction_example.ipynb#scrollTo=uX3xjKZxq5iR*\n","\n"],"metadata":{"id":"34op4-FffmSW"}}]}